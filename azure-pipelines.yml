# Python package
# Create and test a Python package on multiple Python versions.
# Add steps that analyze code, save the dist with the build record, publish to a PyPI-compatible index, and more:
# https://docs.microsoft.com/azure/devops/pipelines/languages/python

variables:
- group: data-platform-kv
- group: databricks

trigger:
- main

pool:
  vmImage: 'ubuntu-latest'
strategy:
  matrix:
    Python37:
      python.version: '3.7'

steps:
- task: UsePythonVersion@0
  inputs:
    versionSpec: '$(python.version)'
  displayName: 'Use Python $(python.version)'

- script: |
    python -m pip install --upgrade pip setuptools wheel twine
    pip install -r requirements.txt
    pip install -r dev_requirements.txt
  displayName: 'Install dependencies'

- script: |
    python setup.py sdist bdist_wheel
    ls dist/
  displayName: 'Artifact creation'

- script: |
    export DATABRICKS_API_HOST=$(WORKSPACE-REGION-URL)
    export DBUTILSTOKEN=$(DATABRICKS-PAT-TOKEN)
    pip install --editable .
    pytest
  displayName: 'Unit Tests'

- task: CopyFiles@2
  inputs:
    SourceFolder: '$(Build.SourcesDirectory)'
    Contents: |
      dist/**
      deployment/**
    TargetFolder: '$(Build.ArtifactStagingDirectory)'

- task: PublishBuildArtifacts@1
  inputs:
    PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    ArtifactName: 'drop'
    publishLocation: 'Container'
  displayName: 'Publish Build Artefacts'

- task: TwineAuthenticate@0
  inputs:
    artifactFeeds: 'sibytes'
    # externalFeeds: 'pypi'
  displayName: 'Authenticate Twine'

- script: |
    twine upload -r sibytes --config-file $(PYPIRC_PATH) $(Build.SourcesDirectory)/dist/*
  continueOnError: true
  displayName: 'Publish to Artefact Store'